{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.python.unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer1 = nn.Linear(37, 1024)\n",
    "        layer2 = nn.Linear(1024, 256)\n",
    "        layer3 = nn.Linear(256, 64)\n",
    "        self.output_layer = nn.Linear(64, 4)\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([layer1, layer2, layer3])\n",
    "        # weight initialization\n",
    "        for l in self.hidden_layers:\n",
    "            nn.init.kaiming_normal_(l.weight, nonlinearity='relu')\n",
    "            nn.init.constant_(l.bias, 0)\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "        nn.init.constant_(self.output_layer.bias, 0)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for l in self.hidden_layers:\n",
    "            x = F.relu(l(x))\n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, sample_size, buffer_size=int(1e4)):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.sample_size = sample_size\n",
    "        self.experience = namedtuple('experience', field_names=('state', 'action', 'reward', 'next_state', 'done'))\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(exp)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.buffer, self.sample_size)\n",
    "        \n",
    "        # divide into batches\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.torchvision.transforms to \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, alpha=1e-3, gamma=0.99, epsilon=0.1, tau=1e-3, action_size=action_size, batch_size=32):\n",
    "        self.Q = DQN().to(device)\n",
    "        self.Q_target = deepcopy(self.Q).to(device)\n",
    "        self.buffer = ReplayBuffer(sample_size=batch_size)\n",
    "        self.t = 0\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.alpha)\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.Q.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.Q(state)\n",
    "        self.Q.train()\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t > 100 + self.buffer.sample_size:\n",
    "            experiences = self.buffer.sample()\n",
    "            self._update_net(experiences)\n",
    "            \n",
    "    def _update_net(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        targets = rewards + (1 - dones) * self.gamma * self.Q_target(next_states).detach().max(1)[0].reshape((-1, 1))\n",
    "        predictions = self.Q(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self._moving_average()\n",
    "    \n",
    "    def _moving_average(self):\n",
    "        for target_param, param in zip(self.Q_target.parameters(), self.Q.parameters()):\n",
    "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Score: -1.0\n",
      "Episode 1 Score: 3.0\n",
      "Episode 2 Score: 0.0\n",
      "Episode 3 Score: 1.0\n",
      "Episode 4 Score: 0.0\n",
      "Episode 5 Score: 0.0\n",
      "Episode 6 Score: -1.0\n",
      "Episode 7 Score: -3.0\n",
      "Episode 8 Score: -1.0\n",
      "Episode 9 Score: 2.0\n",
      "Episode 10 Score: -2.0\n",
      "Episode 11 Score: -1.0\n",
      "Episode 12 Score: -2.0\n",
      "Episode 13 Score: 0.0\n",
      "Episode 14 Score: -2.0\n",
      "Episode 15 Score: 2.0\n",
      "Episode 16 Score: -4.0\n",
      "Episode 17 Score: -2.0\n",
      "Episode 18 Score: -3.0\n",
      "Episode 19 Score: -3.0\n",
      "Episode 20 Score: 3.0\n",
      "Episode 21 Score: -4.0\n",
      "Episode 22 Score: -2.0\n",
      "Episode 23 Score: 0.0\n",
      "Episode 24 Score: 1.0\n",
      "Episode 25 Score: -2.0\n",
      "Episode 26 Score: 7.0\n",
      "Episode 27 Score: -3.0\n",
      "Episode 28 Score: -3.0\n",
      "Episode 29 Score: 2.0\n",
      "Episode 30 Score: -1.0\n",
      "Episode 31 Score: 1.0\n",
      "Episode 32 Score: -4.0\n",
      "Episode 33 Score: 0.0\n",
      "Episode 34 Score: -2.0\n",
      "Episode 35 Score: 3.0\n",
      "Episode 36 Score: 4.0\n",
      "Episode 37 Score: 4.0\n",
      "Episode 38 Score: 2.0\n",
      "Episode 39 Score: -2.0\n",
      "Episode 40 Score: 8.0\n",
      "Episode 41 Score: 3.0\n",
      "Episode 42 Score: 6.0\n",
      "Episode 43 Score: 3.0\n",
      "Episode 44 Score: -1.0\n",
      "Episode 45 Score: 4.0\n",
      "Episode 46 Score: 6.0\n",
      "Episode 47 Score: 4.0\n",
      "Episode 48 Score: 3.0\n",
      "Episode 49 Score: 1.0\n",
      "Episode 50 Score: 0.0\n",
      "Episode 51 Score: 5.0\n",
      "Episode 52 Score: 1.0\n",
      "Episode 53 Score: 5.0\n",
      "Episode 54 Score: 2.0\n",
      "Episode 55 Score: 7.0\n",
      "Episode 56 Score: 2.0\n",
      "Episode 57 Score: 6.0\n",
      "Episode 58 Score: 6.0\n",
      "Episode 59 Score: 1.0\n",
      "Episode 60 Score: 1.0\n",
      "Episode 61 Score: 3.0\n",
      "Episode 62 Score: 9.0\n",
      "Episode 63 Score: 5.0\n",
      "Episode 64 Score: 1.0\n",
      "Episode 65 Score: 6.0\n",
      "Episode 66 Score: 2.0\n",
      "Episode 67 Score: 5.0\n",
      "Episode 68 Score: 4.0\n",
      "Episode 69 Score: 3.0\n",
      "Episode 70 Score: 3.0\n",
      "Episode 71 Score: 3.0\n",
      "Episode 72 Score: 3.0\n",
      "Episode 73 Score: 4.0\n",
      "Episode 74 Score: 5.0\n",
      "Episode 75 Score: 1.0\n",
      "Episode 76 Score: 8.0\n",
      "Episode 77 Score: 12.0\n",
      "Episode 78 Score: 7.0\n",
      "Episode 79 Score: 6.0\n",
      "Episode 80 Score: 6.0\n",
      "Episode 81 Score: 8.0\n",
      "Episode 82 Score: 6.0\n",
      "Episode 83 Score: 8.0\n",
      "Episode 84 Score: 1.0\n",
      "Episode 85 Score: 10.0\n",
      "Episode 86 Score: 8.0\n",
      "Episode 87 Score: 11.0\n",
      "Episode 88 Score: 5.0\n",
      "Episode 89 Score: 5.0\n",
      "Episode 90 Score: 5.0\n",
      "Episode 91 Score: 6.0\n",
      "Episode 92 Score: 7.0\n",
      "Episode 93 Score: 11.0\n",
      "Episode 94 Score: 11.0\n",
      "Episode 95 Score: 12.0\n",
      "Episode 96 Score: 14.0\n",
      "Episode 97 Score: 10.0\n",
      "Episode 98 Score: 10.0\n",
      "Episode 99 Score: 6.0\n",
      "Episode 100 Score: 12.0\n",
      "Episode 101 Score: 7.0\n",
      "Episode 102 Score: 5.0\n",
      "Episode 103 Score: 7.0\n",
      "Episode 104 Score: 4.0\n",
      "Episode 105 Score: 9.0\n",
      "Episode 106 Score: 7.0\n",
      "Episode 107 Score: 8.0\n",
      "Episode 108 Score: 5.0\n",
      "Episode 109 Score: 6.0\n",
      "Episode 110 Score: 11.0\n",
      "Episode 111 Score: 10.0\n",
      "Episode 112 Score: 3.0\n",
      "Episode 113 Score: 2.0\n",
      "Episode 114 Score: 12.0\n",
      "Episode 115 Score: 14.0\n",
      "Episode 116 Score: 12.0\n",
      "Episode 117 Score: 0.0\n",
      "Episode 118 Score: 7.0\n",
      "Episode 119 Score: 6.0\n",
      "Episode 120 Score: 10.0\n",
      "Episode 121 Score: 9.0\n",
      "Episode 122 Score: 6.0\n",
      "Episode 123 Score: 9.0\n",
      "Episode 124 Score: 4.0\n",
      "Episode 125 Score: 5.0\n",
      "Episode 126 Score: 2.0\n",
      "Episode 127 Score: 9.0\n",
      "Episode 128 Score: 2.0\n",
      "Episode 129 Score: 6.0\n",
      "Episode 130 Score: 13.0\n",
      "Episode 131 Score: 9.0\n",
      "Episode 132 Score: 3.0\n",
      "Episode 133 Score: 2.0\n",
      "Episode 134 Score: 10.0\n",
      "Episode 135 Score: 5.0\n",
      "Episode 136 Score: 4.0\n",
      "Episode 137 Score: 5.0\n",
      "Episode 138 Score: 9.0\n",
      "Episode 139 Score: 3.0\n",
      "Episode 140 Score: 10.0\n",
      "Episode 141 Score: 8.0\n",
      "Episode 142 Score: 13.0\n",
      "Episode 143 Score: 6.0\n",
      "Episode 144 Score: 8.0\n",
      "Episode 145 Score: 5.0\n",
      "Episode 146 Score: 9.0\n",
      "Episode 147 Score: 9.0\n",
      "Episode 148 Score: 6.0\n",
      "Episode 149 Score: 6.0\n",
      "Episode 150 Score: 8.0\n",
      "Episode 151 Score: 4.0\n",
      "Episode 152 Score: 9.0\n",
      "Episode 153 Score: 5.0\n",
      "Episode 154 Score: 1.0\n",
      "Episode 155 Score: 13.0\n",
      "Episode 156 Score: 6.0\n",
      "Episode 157 Score: 9.0\n",
      "Episode 158 Score: 5.0\n",
      "Episode 159 Score: 8.0\n",
      "Episode 160 Score: 8.0\n",
      "Episode 161 Score: 6.0\n",
      "Episode 162 Score: 3.0\n",
      "Episode 163 Score: 14.0\n",
      "Episode 164 Score: 7.0\n",
      "Episode 165 Score: 4.0\n",
      "Episode 166 Score: 12.0\n",
      "Episode 167 Score: 7.0\n",
      "Episode 168 Score: 4.0\n",
      "Episode 169 Score: 0.0\n",
      "Episode 170 Score: 6.0\n",
      "Episode 171 Score: 13.0\n",
      "Episode 172 Score: 8.0\n",
      "Episode 173 Score: 14.0\n",
      "Episode 174 Score: 7.0\n",
      "Episode 175 Score: 8.0\n",
      "Episode 176 Score: 6.0\n",
      "Episode 177 Score: 12.0\n",
      "Episode 178 Score: 8.0\n",
      "Episode 179 Score: 6.0\n",
      "Episode 180 Score: 6.0\n",
      "Episode 181 Score: 8.0\n",
      "Episode 182 Score: 8.0\n",
      "Episode 183 Score: 10.0\n",
      "Episode 184 Score: 10.0\n",
      "Episode 185 Score: 12.0\n",
      "Episode 186 Score: 6.0\n",
      "Episode 187 Score: 7.0\n",
      "Episode 188 Score: 6.0\n",
      "Episode 189 Score: 13.0\n",
      "Episode 190 Score: 8.0\n",
      "Episode 191 Score: 3.0\n",
      "Episode 192 Score: 11.0\n",
      "Episode 193 Score: 6.0\n",
      "Episode 194 Score: 7.0\n",
      "Episode 195 Score: 6.0\n",
      "Episode 196 Score: 8.0\n",
      "Episode 197 Score: 5.0\n",
      "Episode 198 Score: 5.0\n",
      "Episode 199 Score: 7.0\n",
      "Episode 200 Score: 8.0\n",
      "Episode 201 Score: 14.0\n",
      "Episode 202 Score: 7.0\n",
      "Episode 203 Score: 13.0\n",
      "Episode 204 Score: 8.0\n",
      "Episode 205 Score: 7.0\n",
      "Episode 206 Score: 5.0\n",
      "Episode 207 Score: 6.0\n",
      "Episode 208 Score: 12.0\n",
      "Episode 209 Score: 3.0\n",
      "Episode 210 Score: 9.0\n",
      "Episode 211 Score: 5.0\n",
      "Episode 212 Score: 10.0\n",
      "Episode 213 Score: 9.0\n",
      "Episode 214 Score: 1.0\n",
      "Episode 215 Score: 4.0\n",
      "Episode 216 Score: 5.0\n",
      "Episode 217 Score: 7.0\n",
      "Episode 218 Score: 10.0\n",
      "Episode 219 Score: 13.0\n",
      "Episode 220 Score: 10.0\n",
      "Episode 221 Score: 11.0\n",
      "Episode 222 Score: 8.0\n",
      "Episode 223 Score: 7.0\n",
      "Episode 224 Score: 11.0\n",
      "Episode 225 Score: 9.0\n",
      "Episode 226 Score: 8.0\n",
      "Episode 227 Score: 4.0\n",
      "Episode 228 Score: 12.0\n",
      "Episode 229 Score: 14.0\n",
      "Episode 230 Score: 10.0\n",
      "Episode 231 Score: 15.0\n",
      "Episode 232 Score: 10.0\n",
      "Episode 233 Score: 13.0\n",
      "Episode 234 Score: 4.0\n",
      "Episode 235 Score: 5.0\n",
      "Episode 236 Score: 10.0\n",
      "Episode 237 Score: 16.0\n",
      "Episode 238 Score: 6.0\n",
      "Episode 239 Score: 6.0\n",
      "Episode 240 Score: 13.0\n",
      "Episode 241 Score: 6.0\n",
      "Episode 242 Score: 10.0\n",
      "Episode 243 Score: 2.0\n",
      "Episode 244 Score: 1.0\n",
      "Episode 245 Score: 6.0\n",
      "Episode 246 Score: 3.0\n",
      "Episode 247 Score: 5.0\n",
      "Episode 248 Score: 8.0\n",
      "Episode 249 Score: 4.0\n",
      "Episode 250 Score: 7.0\n",
      "Episode 251 Score: 5.0\n",
      "Episode 252 Score: 3.0\n",
      "Episode 253 Score: 7.0\n",
      "Episode 254 Score: 3.0\n",
      "Episode 255 Score: 8.0\n",
      "Episode 256 Score: 5.0\n",
      "Episode 257 Score: 8.0\n",
      "Episode 258 Score: 10.0\n",
      "Episode 259 Score: 7.0\n",
      "Episode 260 Score: 12.0\n",
      "Episode 261 Score: 9.0\n",
      "Episode 262 Score: 14.0\n",
      "Episode 263 Score: 7.0\n",
      "Episode 264 Score: 13.0\n",
      "Episode 265 Score: 9.0\n",
      "Episode 266 Score: 3.0\n",
      "Episode 267 Score: 2.0\n",
      "Episode 268 Score: 8.0\n",
      "Episode 269 Score: 7.0\n",
      "Episode 270 Score: 8.0\n",
      "Episode 271 Score: 10.0\n",
      "Episode 272 Score: 5.0\n",
      "Episode 273 Score: 11.0\n",
      "Episode 274 Score: 2.0\n",
      "Episode 275 Score: 11.0\n",
      "Episode 276 Score: 4.0\n",
      "Episode 277 Score: 5.0\n",
      "Episode 278 Score: 0.0\n",
      "Episode 279 Score: 10.0\n",
      "Episode 280 Score: 12.0\n",
      "Episode 281 Score: 17.0\n",
      "Episode 282 Score: 11.0\n",
      "Episode 283 Score: 2.0\n",
      "Episode 284 Score: 6.0\n",
      "Episode 285 Score: 15.0\n",
      "Episode 286 Score: 12.0\n",
      "Episode 287 Score: 6.0\n",
      "Episode 288 Score: 8.0\n",
      "Episode 289 Score: 4.0\n",
      "Episode 290 Score: 8.0\n",
      "Episode 291 Score: 6.0\n",
      "Episode 292 Score: 6.0\n",
      "Episode 293 Score: 11.0\n",
      "Episode 294 Score: 14.0\n",
      "Episode 295 Score: 7.0\n",
      "Episode 296 Score: 13.0\n",
      "Episode 297 Score: 15.0\n",
      "Episode 298 Score: 10.0\n",
      "Episode 299 Score: 8.0\n",
      "Episode 300 Score: 5.0\n",
      "Episode 301 Score: 2.0\n",
      "Episode 302 Score: 7.0\n",
      "Episode 303 Score: 8.0\n",
      "Episode 304 Score: 4.0\n",
      "Episode 305 Score: 8.0\n",
      "Episode 306 Score: 10.0\n",
      "Episode 307 Score: 13.0\n",
      "Episode 308 Score: 9.0\n",
      "Episode 309 Score: 4.0\n",
      "Episode 310 Score: 8.0\n",
      "Episode 311 Score: 7.0\n",
      "Episode 312 Score: 8.0\n",
      "Episode 313 Score: 10.0\n",
      "Episode 314 Score: 8.0\n",
      "Episode 315 Score: 7.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-de78b5c066ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# see if episode has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m                             \u001b[0;31m# roll over the state to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4d3c94819ac9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4d3c94819ac9>\u001b[0m in \u001b[0;36m_update_net\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_moving_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p35/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "for i in range(int(1e3)):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0 \n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    \n",
    "    print(\"Episode {} Score: {}\".format(i, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
